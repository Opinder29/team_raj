# -*- coding: utf-8 -*-
"""Try.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OBe8cQMTtii9Xh1Ak5ayDewo_4UvTSD-
"""

# Step 1: Imports & Data Load
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier # Added RandomForestClassifier here

print("\n1. DATA LOADING & INITIAL INSPECTION …………………………………………")

url = "https://drive.google.com/uc?export=download&id=1QBTnXxORRbJzE5Z2aqKHsVqgB7mqowiN"
df = pd.read_csv(url)
print(df.head(3))
print("Shape:", df.shape)

# Check nulls
print(df.isna().sum())
# Fill object columns with mode, number columns with median
for col in df.select_dtypes(include='object').columns:
    df[col] = df[col].fillna(df[col].mode()[0])
for col in df.select_dtypes(include=np.number).columns:
    df[col] = df[col].fillna(df[col].median())

# Outlier removal (IQR method, numeric columns)
num_cols = df.select_dtypes(include=np.number).columns
Q1 = df[num_cols].quantile(0.25)
Q3 = df[num_cols].quantile(0.75)
IQR = Q3 - Q1
mask = ~((df[num_cols] < (Q1 - 1.5 * IQR)) | (df[num_cols] > (Q3 + 1.5 * IQR))).any(axis=1)
df = df[mask]

# Encode categorical columns
from sklearn.preprocessing import LabelEncoder
cat_cols = df.select_dtypes(include='object').columns
le_dict = {}
for col in cat_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    le_dict[col] = le  # Save for later decoding if needed

print(df.head())

# Univariate analysis: Numeric
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
for col in num_cols:
    plt.figure(figsize=(6,3))
    sns.histplot(df[col].dropna(), kde=True)
    plt.title(f'Distribution of {col}')
    plt.show()

if 'Make' in df.columns and 'Electric Range' in df.columns:
    plt.figure(figsize=(12,6))
    sns.boxplot(x='Make', y='Electric Range', data=df)
    plt.xticks(rotation=90)
    plt.title('Electric Range by Make')
    plt.show()

# Pairplot of main variables (sample for large datasets)
sample_df = df.sample(min(1000, len(df)), random_state=42)
if len(num_cols) > 1:
    sns.pairplot(sample_df[num_cols])
    plt.suptitle('Pairplot of Numeric Features', y=1.02)
    plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Assume df is already loaded
num_cols = df.select_dtypes(include=['int64', 'float64']).columns
corr = df[num_cols].corr()

plt.figure(figsize=(10, 7))
sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Heatmap for Numeric Columns')
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

# Example new feature: Vehicle Age (if 'Model Year' exists)
if 'Model Year' in df.columns:
    df['Vehicle_Age'] = 2025 - df['Model Year']

# Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df.drop('Electric Range', axis=1))  # Assume Electric Range is your target

# Feature Selection (Random Forest importance)
y = (df['Electric Range'] > df['Electric Range'].median()).astype(int)  # Binary target
rf_fs = RandomForestClassifier(n_estimators=100, random_state=42)
rf_fs.fit(X_scaled, y)
importances = rf_fs.feature_importances_
top_idx = np.argsort(importances)[::-1][:10]
top_features = df.drop('Electric Range', axis=1).columns[top_idx]
print("Top features:", top_features)

# Feature extraction (PCA)
from sklearn.decomposition import PCA
pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(df[top_features])

import matplotlib.pyplot as plt
plt.figure(figsize=(7,5))
plt.scatter(X_pca[:,0], X_pca[:,1], c=y, cmap='viridis', alpha=0.5)
plt.title("PCA of Top Features")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

from sklearn.model_selection import train_test_split

# Subsample (optional, for balanced classes)
df_balanced = df.groupby(y).apply(lambda x: x.sample(min(len(x), 300), random_state=42)).reset_index(drop=True)
X = df_balanced[top_features]
y_bal = (df_balanced['Electric Range'] > df_balanced['Electric Range'].median()).astype(int)
X_train, X_test, y_train, y_test = train_test_split(X, y_bal, test_size=0.3, random_state=42, stratify=y_bal)

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
import matplotlib.pyplot as plt
import seaborn as sns

# Apply PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_train)

# Plot PCA results
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_train, palette='Set1', s=60)
plt.title("PCA - First 2 Principal Components")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend(title="Electric Vehicle Type") # Note: The legend title 'Cover_Type' might be a copy-paste error from another project. It should ideally reflect the actual target variable name if desired.
plt.grid(True)
plt.tight_layout()
plt.show()

# Apply LDA
# Change n_components to 1 as max_components is min(n_features, n_classes - 1) = min(10, 2 - 1) = 1
lda = LDA(n_components=1)
X_lda = lda.fit_transform(X_train, y_train)

# Plot LDA results
plt.figure(figsize=(8, 6))
# LDA with n_components=1 results in a 1D array. You typically plot this on a line or use a histogram.
# Plotting against a dummy variable or the class label itself can show separation.
# Here, we plot it on the x-axis against a constant y-value or jittered y-values for visualization.
# A more informative plot might be a histogram of LD1 values for each class.
sns.histplot(x=X_lda[:, 0], hue=y_train, kde=True, palette='Set2')
plt.title("LDA - First Linear Discriminant")
plt.xlabel("LD1")
plt.ylabel("Density")
plt.legend(title="Electric Vehicle Type")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

# Store models and results
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000, penalty='l2', random_state=42),
    'SVM': SVC(kernel='rbf', C=1.0, probability=True, random_state=42),
    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42),
    'Naive Bayes': GaussianNB()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n===== {name} =====")
    print(classification_report(y_test, y_pred))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    # ROC-AUC and curve if possible
    if hasattr(model, "predict_proba"):
        proba = model.predict_proba(X_test)[:, 1]
        auc = roc_auc_score(y_test, proba)
        print("ROC-AUC:", auc)
        RocCurveDisplay.from_estimator(model, X_test, y_test)
        plt.title(f"{name} ROC Curve")
        plt.show()
    else:
        print("ROC-AUC not available for this model.")

# Gradient Boosting with Binning
from sklearn.preprocessing import KBinsDiscretizer

binning = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
X_train_binned = binning.fit_transform(X_train)
X_test_binned = binning.transform(X_test)
gbc_bin = GradientBoostingClassifier()
gbc_bin.fit(X_train_binned, y_train)
y_pred_gbc_bin = gbc_bin.predict(X_test_binned)
print("Gradient Boosting (Optimal Binning) Results:\n", classification_report(y_test, y_pred_gbc_bin))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred_gbc_bin))

from sklearn.metrics import roc_auc_score, RocCurveDisplay
import matplotlib.pyplot as plt

models_to_plot = {
    'NB': models['Naive Bayes'],
    'LR': models['Logistic Regression'],
    'SVM': models['SVM'],
    'GBC': models['Gradient Boosting'],
    'GBC_bin': gbc_bin # gbc_bin was defined in the previous cell (ipython-input-11)
}

for name, model in models_to_plot.items():
    if hasattr(model, "predict_proba"):
        RocCurveDisplay.from_estimator(model, X_test, y_test)
        plt.title(name + " ROC Curve")
        plt.show()
        print(f"{name} ROC-AUC:", roc_auc_score(y_test, model.predict_proba(X_test)[:,1]))
    elif hasattr(model, "decision_function"):
        RocCurveDisplay.from_estimator(model, X_test, y_test)
        plt.title(name + " ROC Curve")
        plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from scipy.stats import uniform, randint

# Use a smaller subset for tuning (optional, but helps)
X_sample = X_train.sample(n=min(2000, len(X_train)), random_state=42)
y_sample = y_train.loc[X_sample.index]

# Parameter distributions
param_dist_lr = {
    'C': uniform(0.01, 10),
    'penalty': ['l2'],
    'solver': ['lbfgs']
}
param_dist_svm = {
    'C': uniform(0.1, 10)
}
param_dist_gbc = {
    'n_estimators': randint(50, 200),
    'learning_rate': uniform(0.01, 0.2),
    'max_depth': randint(3, 7)
}
param_dist_nb = {}

n_iter_search = 10  # Try 10 random combinations per model

# Logistic Regression
rs_lr = RandomizedSearchCV(
    LogisticRegression(max_iter=1000, random_state=42),
    param_distributions=param_dist_lr,
    n_iter=n_iter_search, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)
rs_lr.fit(X_sample, y_sample)
print("Best Logistic Regression params:", rs_lr.best_params_)

# SVM

# Run randomized search for SVM
# The original code defines rs_svm_linear but never fits it and then tries to access rs_svm.best_estimator_
# Let's assume the user intended to run RandomizedSearchCV for the general SVM param_dist_svm
rs_svm = RandomizedSearchCV(
    SVC(random_state=42, max_iter=5000),
    param_distributions=param_dist_svm, # Use the general SVM parameter distribution
    n_iter=5,  # Use n_iter_search for consistency
    cv=2,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42
)
rs_svm.fit(X_sample, y_sample) # Fit the SVM RandomizedSearchCV
print("Best SVM params:", rs_svm.best_params_)


# Gradient Boosting
rs_gbc = RandomizedSearchCV(
    # Removed n_bins, encode, strategy as they are not arguments for GBC
    GradientBoostingClassifier(random_state = 42),
    param_distributions=param_dist_gbc,
    n_iter=n_iter_search, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)
rs_gbc.fit(X_sample, y_sample)
print("Best Gradient Boosting params:", rs_gbc.best_params_)

# Naive Bayes (no real params, but for consistency)
rs_nb = RandomizedSearchCV(
    GaussianNB(), param_distributions=param_dist_nb,
    n_iter=1, cv=3, scoring='accuracy', random_state=42)
rs_nb.fit(X_sample, y_sample)
print("Best Naive Bayes params:", rs_nb.best_params_) # Print best params for NB as well

# Evaluate best estimators on full test set
print("\n--- Test Set Evaluation ---")
print("LR Test Accuracy:", rs_lr.best_estimator_.score(X_test, y_test))
print("SVM Test Accuracy:", rs_svm.best_estimator_.score(X_test, y_test)) # Use rs_svm
print("GBC Test Accuracy:", rs_gbc.best_estimator_.score(X_test, y_test))
print("NB Test Accuracy:", rs_nb.best_estimator_.score(X_test, y_test))

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2, random_state=42)
X_pca = pca.fit_transform(X)

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=y_bal, cmap='coolwarm', alpha=0.6)
plt.title("PCA Projection of Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.colorbar(label='Class')
plt.show()

from sklearn.manifold import TSNE

# t-SNE on top features
tsne = TSNE(n_components=2, random_state=42)
X_tsne = tsne.fit_transform(X)

plt.figure(figsize=(8,5))
# Use y_bal for coloring as it corresponds to the subsampled data X
plt.scatter(X_tsne[:,0], X_tsne[:,1], c=y_bal, cmap='plasma', alpha=0.7)
plt.title("t-SNE of Features")
plt.xlabel("t-SNE1")
plt.ylabel("t-SNE2")
plt.show()

